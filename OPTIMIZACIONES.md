# üöÄ Optimizaciones Implementadas y Futuras

## ‚úÖ Optimizaciones Actuales (Implementadas)

### 1. **Cach√© Persistente de Vectorstore** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

- **Mejora:** 84.3% m√°s r√°pido en consultas subsecuentes
- **Ubicaci√≥n:** `./chroma_db/{category}`
- **Funcionamiento:** Los embeddings se generan una sola vez y se reutilizan
- **Invalidaci√≥n:** Autom√°tica al agregar/modificar PDFs

### 2. **Streaming de Respuestas** üåä

- **Mejora:** TTFB 14.5% m√°s r√°pido
- **Endpoint:** `POST /ask-stream`
- **Beneficio:** El usuario ve contenido inmediatamente
- **Formato:** Server-Sent Events (SSE)

### 3. **L√≠mite de Documentos Recuperados**

- **Configuraci√≥n:** `k=4` documentos m√°s relevantes
- **Beneficio:** Reduce tokens enviados a GPT
- **Resultado:** Respuestas m√°s r√°pidas y econ√≥micas

### 4. **Chunking Optimizado**

- **Tama√±o de chunk:** 1000 caracteres
- **Overlap:** 200 caracteres
- **Beneficio:** Balance entre contexto y precisi√≥n

---

## üîÆ Optimizaciones Futuras Recomendadas

### 1. **Implementar Redis para Cach√© de Respuestas** ‚ö°

**Impacto esperado:** 95%+ m√°s r√°pido para preguntas repetidas

```python
import redis
from functools import lru_cache
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_cached_answer(question: str, category: str):
    cache_key = hashlib.md5(f"{question}:{category}".encode()).hexdigest()
    cached = redis_client.get(cache_key)
    if cached:
        return cached.decode('utf-8')
    return None

def cache_answer(question: str, category: str, answer: str):
    cache_key = hashlib.md5(f"{question}:{category}".encode()).hexdigest()
    redis_client.setex(cache_key, 3600, answer)  # Cache 1 hora
```

**Instalaci√≥n:**

```bash
pip install redis
brew install redis  # macOS
redis-server
```

---

### 2. **Usar Embeddings M√°s R√°pidos** üèÉ‚Äç‚ôÇÔ∏è

**Impacto esperado:** 30-40% m√°s r√°pido en primera carga

**Opci√≥n A: text-embedding-3-small (OpenAI)**

```python
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
# M√°s r√°pido y econ√≥mico que ada-002
```

**Opci√≥n B: Sentence Transformers (local)**

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
# GRATIS, sin l√≠mites de API, m√°s r√°pido
```

---

### 3. **Procesamiento As√≠ncrono de PDFs** üîÑ

**Impacto esperado:** No bloquea requests mientras se procesan PDFs

```python
from fastapi import BackgroundTasks
import asyncio

@app.post("/index-category")
async def index_category(category: str, background_tasks: BackgroundTasks):
    background_tasks.add_task(process_category_async, category)
    return {"status": "indexing", "category": category}

async def process_category_async(category: str):
    # Procesar PDFs en background
    documents = load_documents_from_category(category)
    # ... resto del procesamiento
```

---

### 4. **Reranking de Resultados** üéØ

**Impacto esperado:** Mejor calidad con menos documentos (k=2 en vez de k=4)

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
```

**Instalaci√≥n:**

```bash
pip install langchain-cohere  # o usar crossencoder
```

---

### 5. **Paralelizaci√≥n de Carga de PDFs** üîÄ

**Impacto esperado:** 50%+ m√°s r√°pido en primera indexaci√≥n

```python
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

def load_pdf_parallel(pdf_file):
    loader = PyPDFLoader(pdf_file)
    return loader.load()

def load_documents_from_category(category: str):
    pdf_files = glob.glob(os.path.join(docs_path, "*.pdf"))

    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
        documents_lists = executor.map(load_pdf_parallel, pdf_files)

    documents = []
    for doc_list in documents_lists:
        documents.extend(doc_list)

    return documents
```

---

### 6. **Compresi√≥n de Contexto** üì¶

**Impacto esperado:** 40% menos tokens, respuestas m√°s r√°pidas

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Resumir contexto antes de enviarlo a GPT
compression_prompt = PromptTemplate(
    template="Resume este texto manteniendo solo informaci√≥n relevante para: {question}\n\nTexto: {context}",
    input_variables=["question", "context"]
)
```

---

### 7. **√çndice H√≠brido (Denso + Sparse)** üîç

**Impacto esperado:** Mejor precisi√≥n en b√∫squedas t√©cnicas

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# Combinar b√∫squeda sem√°ntica (embeddings) con keyword (BM25)
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 4

ensemble_retriever = EnsembleRetriever(
    retrievers=[retriever, bm25_retriever],
    weights=[0.7, 0.3]  # 70% sem√°ntico, 30% keywords
)
```

---

### 8. **Modelo M√°s Peque√±o para Consultas Simples** üí∞

**Impacto esperado:** 70% m√°s econ√≥mico, 2x m√°s r√°pido

```python
from langchain_openai import ChatOpenAI

# Detectar complejidad de pregunta
def get_appropriate_model(question: str):
    simple_keywords = ["qu√© es", "define", "cu√°ntos", "cu√°l es"]
    if any(kw in question.lower() for kw in simple_keywords):
        return ChatOpenAI(model="gpt-4o-mini")  # M√°s r√°pido
    else:
        return ChatOpenAI(model="gpt-4o-2024-08-06")  # M√°s potente
```

---

### 9. **Pre-calentamiento de Cach√©** üî•

**Impacto esperado:** Primera consulta tambi√©n r√°pida

```python
@app.on_event("startup")
async def startup_event():
    # Pre-cargar vectorstores en memoria al iniciar
    categories = ["geomecanica"]
    for category in categories:
        try:
            get_or_create_vectorstore(category)
            print(f"‚úÖ Vectorstore pre-cargado para '{category}'")
        except Exception as e:
            print(f"‚ö†Ô∏è Error pre-cargando '{category}': {e}")
```

---

### 10. **Monitoreo y M√©tricas** üìä

**Impacto:** Identificar nuevos cuellos de botella

```python
import time
from functools import wraps

def timing_decorator(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start = time.time()
        result = await func(*args, **kwargs)
        duration = time.time() - start
        print(f"‚è±Ô∏è {func.__name__}: {duration:.2f}s")
        return result
    return wrapper

@app.post("/ask")
@timing_decorator
async def ask_question(question_request: QuestionRequest):
    # ... c√≥digo existente
```

---

## üìà Roadmap de Implementaci√≥n Sugerido

### Fase 1 (Ganancias Inmediatas) - 1-2 horas

1. ‚úÖ Cach√© persistente (HECHO)
2. ‚úÖ Streaming (HECHO)
3. ‚¨ú Embeddings text-embedding-3-small
4. ‚¨ú Redis para cach√© de respuestas

### Fase 2 (Optimizaciones Medias) - 3-4 horas

5. ‚¨ú Paralelizaci√≥n de PDFs
6. ‚¨ú Modelo adaptativo (mini vs full)
7. ‚¨ú Pre-calentamiento de cach√©

### Fase 3 (Optimizaciones Avanzadas) - 1-2 d√≠as

8. ‚¨ú Reranking con Cohere
9. ‚¨ú √çndice h√≠brido BM25 + Embeddings
10. ‚¨ú Compresi√≥n de contexto
11. ‚¨ú Monitoreo y m√©tricas

---

## üéØ Prioridad por ROI (Return on Investment)

| Optimizaci√≥n           | Esfuerzo | Ganancia | ROI        | Prioridad |
| ---------------------- | -------- | -------- | ---------- | --------- |
| Redis cach√©            | 30 min   | 95%      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üî• Alta   |
| text-embedding-3-small | 5 min    | 40%      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üî• Alta   |
| Modelo adaptativo      | 15 min   | 50%      | ‚≠ê‚≠ê‚≠ê‚≠ê   | üî• Alta   |
| Paralelizaci√≥n PDFs    | 45 min   | 50%      | ‚≠ê‚≠ê‚≠ê‚≠ê   | Media     |
| Pre-calentamiento      | 10 min   | 30%      | ‚≠ê‚≠ê‚≠ê‚≠ê   | Media     |
| Reranking              | 2 horas  | 25%      | ‚≠ê‚≠ê‚≠ê     | Baja      |
| √çndice h√≠brido         | 3 horas  | 20%      | ‚≠ê‚≠ê       | Baja      |

---

## üõ†Ô∏è Herramientas Recomendadas

### Para Monitoreo:

- **Prometheus + Grafana** - M√©tricas en tiempo real
- **Sentry** - Tracking de errores
- **Langsmith** - Debugging de LangChain

### Para Testing:

- **pytest** - Tests automatizados
- **locust** - Load testing
- **httpx** - Cliente async para benchmarks

---

## üìö Recursos Adicionales

- [LangChain Performance Best Practices](https://python.langchain.com/docs/guides/performance)
- [Chroma Performance Tuning](https://docs.trychroma.com/usage-guide)
- [FastAPI Async Best Practices](https://fastapi.tiangolo.com/async/)
- [OpenAI Embeddings Comparison](https://platform.openai.com/docs/guides/embeddings)

---

**√öltima actualizaci√≥n:** 23 de octubre de 2025
